{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T18:33:26.084350Z",
     "start_time": "2019-05-21T18:33:24.768501Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gc\n",
    "import scipy.sparse as sparse\n",
    "import re\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T18:36:44.566208Z",
     "start_time": "2019-05-21T18:36:44.463732Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"data/cookingClean/Cooking_Train.pkl\", 'rb') as f:\n",
    "    df_train = pickle.load(f)\n",
    "with open(\"data/cookingClean/Cooking_Test.pkl\", 'rb') as f:\n",
    "    df_test = pickle.load(f)\n",
    "with open(\"data/cookingClean/Cooking_Valid.pkl\", 'rb') as f:\n",
    "    df_valid = pickle.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the intersection of the labels because no one is included in each other etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T18:50:48.404731Z",
     "start_time": "2019-05-21T18:50:48.398859Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = list(set(list(df_train)) & set(list(df_valid)) & set(list(df_test)))\n",
    "labels.remove(\"text\")\n",
    "labels = [\"text\"] + labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T18:51:31.613567Z",
     "start_time": "2019-05-21T18:51:31.582613Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train = df_train[labels]\n",
    "df_test = df_test[labels]\n",
    "df_valid = df_valid[labels]\n",
    "\n",
    "labels = labels[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model using TFIDF and logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T18:36:45.907331Z",
     "start_time": "2019-05-21T18:36:45.903080Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stpW = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-21T18:37:38.620640Z",
     "start_time": "2019-05-21T18:37:36.299936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 s, sys: 10.4 ms, total: 1.07 s\n",
      "Wall time: 1.07 s\n",
      "CPU times: user 122 ms, sys: 111 Âµs, total: 122 ms\n",
      "Wall time: 121 ms\n",
      "CPU times: user 141 ms, sys: 0 ns, total: 141 ms\n",
      "Wall time: 141 ms\n"
     ]
    }
   ],
   "source": [
    "transform_com = TfidfVectorizer(analyzer = 'char', ngram_range=(1,4), max_features=50000,stop_words=stpW ,min_df=2).fit(df_train.text)\n",
    "\n",
    "\n",
    "%time comments_train = transform_com.transform(df_train.text)\n",
    "%time comments_test = transform_com.transform(df_test.text)\n",
    "%time comments_valid = transform_com.transform(df_valid.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-21T18:52:52.016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "preds = np.zeros((df_test.shape[0], len(labels)))\n",
    "model =  LogisticRegression(penalty = 'l2', C= 9.0)\n",
    "\n",
    "for i, j in enumerate(labels[1:]):\n",
    "    print('fit '+j)\n",
    "    model.fit(comments_train, df_train[j])\n",
    "    pred_train = model.predict_proba(comments_train)[:,1]\n",
    "    print('Training loss: {}'.format(log_loss(df_train[j], pred_train)))\n",
    "    preds[:,i] = model.predict_proba(comments_test)[:,1]\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-21T18:56:58.519Z"
    }
   },
   "source": [
    "## Second modele using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PARAMETERS\n",
    "MAX_NB_WORDS = 20000\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "embedding_vector_length = 100\n",
    "batch_size = 128\n",
    "nb_epochs = 10\n",
    "\n",
    "\n",
    "comments = df_train.text.tolist() + df_test.text.tolist() + df_valid.text.tolist()\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, char_level=False)\n",
    "tokenizer.fit_on_texts(comments)\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(df_train.comment_text)\n",
    "sequences_test = tokenizer.texts_to_sequences(df_test.comment_text)\n",
    "sequences_valid = tokenizer.texts_to_sequences(df_valid.comment_text)\n",
    "\n",
    "x_train = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_valid = pad_sequences(sequences_valid, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "\n",
    "print('Shape of train tensor ', x_train.shape)\n",
    "print('Shape of test tensor ', x_test.shape)\n",
    "print('Shape of valid tensor ', x_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A transformer en pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_NB_WORDS, embedding_vector_length, input_length=MAX_SEQUENCE_LENGTH))\n",
    "    model.add(LSTM(100, return_sequences = True, dropout = 0.2))\n",
    "    model.add(GlobalMaxPool1D())\n",
    "    model.add(Dense(100,  activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(50,  activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(6, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
